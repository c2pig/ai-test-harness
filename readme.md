# AI Test Harness

[![CI](https://github.com/c2pig/ai-test-harness/actions/workflows/ci.yml/badge.svg)](https://github.com/c2pig/ai-test-harness/actions/workflows/ci.yml)
[![npm version](https://badge.fury.io/js/%40ai-test-harness%2Fcore.svg)](https://www.npmjs.com/package/@ai-test-harness/core)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Node Version](https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen)](https://nodejs.org)

Multi-tenant AI quality assessment framework for testing and evaluating LLM applications, with first-class support for AWS Bedrock.

## Features

- ğŸ¯ **Multi-tenant testing** - Organize tests by use case or product area
- ğŸ¤– **AWS Bedrock integration** - Native support for Bedrock models and agents
- âœ… **Quality assessment** - Built-in framework for evaluating AI outputs
- ğŸ“Š **Data connectors** - CloudWatch, DynamoDB, PostgreSQL integration
- ğŸ” **Schema validation** - Strong typing with Zod schemas
- ğŸ› ï¸ **CLI scaffolding** - Interactive tools to create and manage tests
- ğŸ“ˆ **Flexible evaluation** - Configurable quality attributes and scoring

## Installation

### As a library

```bash
npm install @ai-test-harness/core
```

### As a CLI tool

```bash
npm install -g @ai-test-harness/cli
```

Or use in your project:

```bash
npm install --save-dev @ai-test-harness/cli
```

## Quick Start

Command-line utilities for managing the testing framework. These tools help you create tenants, validate configs, check schema versions, and clean up outputs.

## Quick Reference

| Command | npm script | What it does |
|---------|-----------|-------------|
| create-tenant | `npm run create-tenant` | Interactive wizard to create new tenant with configs |
| validate-configs | `npm run validate` | Validate all configuration files using Zod schemas |
| show-schema-version | `npm run schema:version` | Display current schema version and compatibility |
| clean:outputs | `npm run clean:outputs` | Remove all output folders from tenants |

---


## Common Workflows

### Creating and running a new test suite

```bash
# 1. Create tenant
npm run create-tenant
# Follow cli prompt instruction
# << your input is "my-tenant"
# 2. Edit config
vi tenants/my-tenant/config.yaml

# 3. Validate before running
npm run validate

# 4. Run tests
npm run test:my-tenant
```

### Validating configs before commit

```bash
# Good practice: always validate before committing
npm run validate


### Cleaning up after testing

```bash
# Clean all outputs
npm run clean:outputs
# Output: "All output folders cleaned"
```

### Checking schema compatibility

```bash
# Check current schema version
npm run schema:version

# If you're upgrading, check docs first
cat docs/config-schema.md
```

## create-tenant

Creates a new tenant directory with all necessary configuration files. This is the starting point for any new test suite.

### Usage

```bash
npm run create-tenant
```

The script will prompt you for:
1. **Tenant name** (kebab-case, e.g., "job-description")
2. **Description** (short description of what this tenant tests)
3. **Test type**:
   - `text-generation`: LLM prompt-based generation with quality assessment
   - `agent-scenario`: Bedrock Agent conversational testing with scenarios

### What it creates

For **text-generation** tenants:
```
tenants/your-tenant-name/
â”œâ”€â”€ config.yaml              # Main configuration
â””â”€â”€ evaluation/
    â””â”€â”€ calibration.yaml     # Calibration examples for judge
```

For **agent-scenario** tenants:
```
tenants/your-tenant-name/
â”œâ”€â”€ config.yaml              # Main configuration
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ calibration.yaml     # Calibration examples for judge
â””â”€â”€ scenarios/
    â”œâ”€â”€ positive-tests/
    â”‚   â””â”€â”€ sample-scenario.yaml
    â””â”€â”€ negative-tests/
```

---

## validate-configs

Validates all configuration files in the repository using Zod schemas. This catches errors before you run tests.

### Usage

```bash
npm run validate
```

This command runs both:
- Type checking (`tsc --noEmit`)
- Configuration validation (Zod schema validation)

### What it validates

The validator runs **7 validation phases**:

1. **Shared Models** (`config/shared/models.yaml`)
   - Validates all model definitions (bedrock-agent, prompt types)
   - Checks required fields (agentId, promptPath, inputKeys, etc.)

2. **Shared Connectors** (`config/shared/connectors.yaml`)
   - Validates all connector definitions (cloudwatch, dynamodb, postgresql, etc.)
   - Checks outputSchema is defined for data connectors
   - Validates recordKey is specified

3. **Framework Configs**
   - Checks framework prompt files exist

4. **Tenant Configs** (`tenants/*/config.yaml`)
   - Validates schema version compatibility
   - Checks config structure matches test type
   - Validates quality attributes are defined
   - Ensures all required fields are present

5. **Scenario Files** (`tenants/*/scenarios/**/*.yaml`)
   - Validates scenario structure
   - Checks required fields (scenarioId, description, conversationExamples)
   - Validates tool call expectations format

6. **Calibration Files** (`tenants/*/evaluation/calibration.yaml`)
   - Validates calibration example structure
   - Checks grade-score mappings

7. **Cross-References**
   - Verifies referenced models exist in models.yaml
   - Verifies referenced connectors exist in connectors.yaml
   - Ensures no dangling references

### Example output

**Success:**
```
=== Configuration Validator ===

[1/7] Validating shared configuration files...
  âœ“ models.yaml: 5 models defined
  âœ“ connectors.yaml: 8 connectors defined

[2/7] Checking framework configs...
  âœ“ Found 3 framework prompt files

[3/7] Validating tenant configuration files...
  Found 3 tenant configs
  âœ“ product-listing: valid (schemaVersion: 1.0.0)
  âœ“ product-outreach-scenario: valid (schemaVersion: 1.0.0)

[4/7] Validating scenario files...
  âœ“ 12 scenario files validated

[5/7] Validating calibration files...
  âœ“ 3 calibration files validated

[6/7] Validating cross-references...
  âœ“ All model references valid
  âœ“ All connector references valid

[7/7] Summary
  âœ“ All validations passed
```

**Failure:**
```
=== Configuration Validator ===

[1/7] Validating shared configuration files...
  âœ— connectors.yaml [connectors.prod-cloudwatch]: recordKey is required

[3/7] Validating tenant configuration files...
  âœ— product-listing:
     [evaluationPlan.judgeModel.maxTokens]: Expected number, got undefined

âœ— Validation failed with 2 errors
```

### Exit codes

- **0** = All validations passed
- **1** = Validation failed (errors found)

Use this in CI/CD pipelines to prevent deploying broken configs.

---

## show-schema-version

Displays the current schema version and all supported versions. Use this to check compatibility when upgrading configs.

### Usage

```bash
npm run schema:version
```

### Example output

```
============================================================
Configuration Schema Version
============================================================

Current Version: 1.0.0

Supported Versions:
  â†’ 1.0.0 (current)

For schema documentation, see:
  docs/config-schema.md
```

### When to use this

- Before upgrading the framework to check if your configs need migration
- When debugging schema version errors
- To verify which schema features are available

---

## clean:outputs

Removes all output folders from tenant directories. Use this to free up disk space or start fresh.

### Usage

```bash
npm run clean:outputs
```

## Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## Packages

This monorepo contains:

- **[@ai-test-harness/core](./packages/core)** - Core testing framework
- **[@ai-test-harness/cli](./packages/cli)** - CLI tool for scaffolding and running tests

## Documentation

- [Configuration Schema](./docs/config-schema.md)
- [Contributing Guide](./CONTRIBUTING.md)
- [Changelog](./CHANGELOG.md)

## Requirements

- Node.js >= 18.0.0
- npm >= 8.0.0

## License

[MIT](LICENSE) Â© AI Test Harness Contributors

## Support

- [GitHub Issues](https://github.com/c2pig/ai-test-harness/issues)
- [Documentation](https://github.com/c2pig/ai-test-harness#readme)